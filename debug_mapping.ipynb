{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "982b71e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "def create_department_csv(department_code):\n",
    "    \"\"\"\n",
    "    Creates a CSV file containing all communes for a given department.\n",
    "    \n",
    "    Args:\n",
    "        department_code (str): The department code (e.g., '06', '75', '2A')\n",
    "    \n",
    "    Returns:\n",
    "        str: The path of the created file\n",
    "    \"\"\"\n",
    "    # Read source file\n",
    "    # Specify dtypes for code columns to preserve leading zeros and avoid .0 suffix\n",
    "    dtype_dict = {\n",
    "        'code_insee': str,\n",
    "        'dep_code': str,\n",
    "        'code_postal': str,\n",
    "        'codes_postaux': str,\n",
    "        'reg_code': str,\n",
    "        'canton_code': str,\n",
    "        'epci_code': str\n",
    "    }\n",
    "    \n",
    "    df = pd.read_csv('communes-france-2025.csv', dtype=dtype_dict, low_memory=False)\n",
    "    \n",
    "    # Filter by department\n",
    "    df_dept = df[df['dep_code'] == str(department_code)]\n",
    "    \n",
    "    # Create directory if not exists\n",
    "    os.makedirs('departements', exist_ok=True)\n",
    "    \n",
    "    # Create filename\n",
    "    output_file = f'departements/communes-departement{department_code}.csv'\n",
    "    \n",
    "    # Save file\n",
    "    df_dept.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"CSV created: {output_file} ({len(df_dept)} communes)\")\n",
    "    \n",
    "    return output_file\n",
    "\n",
    "\n",
    "def create_department_json(department_code):\n",
    "    \"\"\"\n",
    "    Creates a JSON file containing all gas stations for a given department.\n",
    "    \n",
    "    Args:\n",
    "        department_code (str): The department code (e.g., '06', '75', '2A')\n",
    "    \n",
    "    Returns:\n",
    "        str: The path of the created file\n",
    "    \"\"\"\n",
    "    # Read source JSON file\n",
    "    with open('prix-carburants-quotidien-preprocessed.json', 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Filter stations by department code\n",
    "    filtered_data = [station for station in data if station.get('dep_code') == str(department_code)]\n",
    "    \n",
    "    # Create directory if not exists\n",
    "    os.makedirs('departements', exist_ok=True)\n",
    "    \n",
    "    # Create filename\n",
    "    output_file = f'departements/stations-departement{department_code}.json'\n",
    "    \n",
    "    # Save file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(filtered_data, f, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"JSON created: {output_file} ({len(filtered_data)} stations)\")\n",
    "    \n",
    "    return output_file\n",
    "\n",
    "\n",
    "def create_department_files(department_code):\n",
    "    \"\"\"\n",
    "    Creates both CSV and JSON files for a given department.\n",
    "    \n",
    "    Args:\n",
    "        department_code (str): The department code (e.g., '06', '75', '2A')\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (csv_file_path, json_file_path)\n",
    "    \"\"\"\n",
    "    csv_file = create_department_csv(department_code)\n",
    "    json_file = create_department_json(department_code)\n",
    "    return csv_file, json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10c65bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "def run_mapping(department_code):\n",
    "    \"\"\"\n",
    "    Runs the RML mapping for a specific department.\n",
    "    \n",
    "    Args:\n",
    "        department_code (str): The department code.\n",
    "    \"\"\"\n",
    "    # 1. Create the department CSV and JSON files\n",
    "    csv_file_path, json_file_path = create_department_files(department_code)\n",
    "    \n",
    "    # 2. Update the mapping file\n",
    "    mapping_file = 'mapping-optimized.ttl'\n",
    "    \n",
    "    # Ensure mappings directory exists\n",
    "    os.makedirs('mappings', exist_ok=True)\n",
    "    temp_mapping_file = f'mappings/mapping_{department_code}.ttl'\n",
    "    \n",
    "    with open(mapping_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Replace CSV source file (communes)\n",
    "    new_content = re.sub(\n",
    "        r'rml:source\\s+\"[^\"]+\\.csv\"\\s*;',\n",
    "        f'rml:source \"{csv_file_path}\" ;',\n",
    "        content\n",
    "    )\n",
    "    \n",
    "    # Replace JSON source file (stations)\n",
    "    new_content = re.sub(\n",
    "        r'rml:source\\s+\"[^\"]+\\.json\"\\s*;',\n",
    "        f'rml:source \"{json_file_path}\" ;',\n",
    "        new_content\n",
    "    )\n",
    "    \n",
    "    with open(temp_mapping_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(new_content)\n",
    "        \n",
    "    print(f\"Mapping created: {temp_mapping_file}\")\n",
    "    \n",
    "    # 3. Run the Docker command\n",
    "    # Ensure outputs directory exists\n",
    "    os.makedirs('outputs', exist_ok=True)\n",
    "    output_file = f'outputs/output_{department_code}.ttl'\n",
    "    \n",
    "    print(f\"Running RML mapper for department {department_code}...\")\n",
    "    \n",
    "    # Get absolute path for volume mount\n",
    "    current_dir = os.getcwd()\n",
    "    \n",
    "    # Docker command\n",
    "    cmd = [\n",
    "        \"docker\", \"run\", \"--rm\",\n",
    "        \"-v\", f\"{current_dir}:/data\",\n",
    "        \"-w\", \"/data\",\n",
    "        \"rmlio/rmlmapper-java:latest\",\n",
    "        \"--mappingfile\", temp_mapping_file,\n",
    "        \"--outputfile\", output_file,\n",
    "        \"--serialization\", \"turtle\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            cmd,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "        \n",
    "        return_code = result.returncode\n",
    "        \n",
    "        if return_code == 0:\n",
    "            if os.path.exists(output_file):\n",
    "                print(f\"Mapping completed: {output_file}\")\n",
    "            else:\n",
    "                print(\"Error: Docker command succeeded but output file was not created.\")\n",
    "        else:\n",
    "            print(f\"Docker execution failed with return code: {return_code}\")\n",
    "            print(\"Error output:\")\n",
    "            print(result.stderr)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during execution: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cc6c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load departments list\n",
    "df_communes = pd.read_csv('communes-france-2025.csv')\n",
    "# Get unique department codes and sort them\n",
    "departments = sorted(df_communes['dep_code'].astype(str).unique().tolist())\n",
    "\n",
    "# Dictionary to store results: dep_code -> (time_taken, line_count)\n",
    "results = {}\n",
    "\n",
    "# Select a subset for demonstration (e.g., first 5) to avoid long execution time\n",
    "# Uncomment the next line to run for all departments\n",
    "# target_departments = departments \n",
    "target_departments = departments\n",
    "\n",
    "print(f\"Running benchmark for {len(target_departments)} departments...\")\n",
    "\n",
    "for dep in tqdm(target_departments):\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    # Run the mapping\n",
    "    run_mapping(dep)\n",
    "    \n",
    "    end_time = time.perf_counter()\n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "    # Count lines in output file\n",
    "    output_file = f'outputs/output_{dep}.ttl'\n",
    "    line_count = 0\n",
    "    if os.path.exists(output_file):\n",
    "        try:\n",
    "            with open(output_file, 'r', encoding='utf-8') as f:\n",
    "                line_count = sum(1 for _ in f)\n",
    "        except Exception as e:\n",
    "            print(f\"Error counting lines: {e}\")\n",
    "    \n",
    "    results[dep] = (execution_time, line_count)\n",
    "    print(f\"Department {dep}: {execution_time:.4f}s, {line_count} lines\")\n",
    "\n",
    "print(\"\\nBenchmark completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0212ce69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results sorted by execution time:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Department</th>\n",
       "      <th>Time (s)</th>\n",
       "      <th>Lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>27</td>\n",
       "      <td>10.667438</td>\n",
       "      <td>7842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>25</td>\n",
       "      <td>10.436756</td>\n",
       "      <td>7851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>62</td>\n",
       "      <td>10.178413</td>\n",
       "      <td>14534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>9.958487</td>\n",
       "      <td>11584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>59</td>\n",
       "      <td>9.707620</td>\n",
       "      <td>13328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>972</td>\n",
       "      <td>5.247481</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>971</td>\n",
       "      <td>5.210229</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>976</td>\n",
       "      <td>5.191615</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>973</td>\n",
       "      <td>5.160111</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>974</td>\n",
       "      <td>5.055132</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Department   Time (s)  Lines\n",
       "25          27  10.667438   7842\n",
       "23          25  10.436756   7851\n",
       "62          62  10.178413  14534\n",
       "33          33   9.958487  11584\n",
       "59          59   9.707620  13328\n",
       "..         ...        ...    ...\n",
       "97         972   5.247481    278\n",
       "96         971   5.210229    262\n",
       "100        976   5.191615    142\n",
       "98         973   5.160111    182\n",
       "99         974   5.055132    198\n",
       "\n",
       "[101 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a DataFrame from the results\n",
    "data = []\n",
    "for dep, (exec_time, lines) in results.items():\n",
    "    data.append({\n",
    "        'Department': dep,\n",
    "        'Time (s)': exec_time,\n",
    "        'Lines': lines\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(data)\n",
    "\n",
    "# Sort by Time\n",
    "df_results_sorted = df_results.sort_values(by='Time (s)', ascending=False)\n",
    "\n",
    "# Display the sorted table\n",
    "print(\"Results sorted by execution time:\")\n",
    "try:\n",
    "    display(df_results_sorted)\n",
    "except NameError:\n",
    "    print(df_results_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d269e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 101 files to merge...\n",
      "Processing output_01.ttl...\n",
      "Processing output_02.ttl...\n",
      "Processing output_03.ttl...\n",
      "Processing output_04.ttl...\n",
      "Processing output_05.ttl...\n",
      "Processing output_06.ttl...\n",
      "Processing output_07.ttl...\n",
      "Processing output_08.ttl...\n",
      "Processing output_09.ttl...\n",
      "Processing output_10.ttl...\n",
      "Processing output_11.ttl...\n",
      "Processing output_12.ttl...\n",
      "Processing output_13.ttl...\n",
      "Processing output_14.ttl...\n",
      "Processing output_15.ttl...\n",
      "Processing output_16.ttl...\n",
      "Processing output_17.ttl...\n",
      "Processing output_18.ttl...\n",
      "Processing output_19.ttl...\n",
      "Processing output_21.ttl...\n",
      "Processing output_22.ttl...\n",
      "Processing output_23.ttl...\n",
      "Processing output_24.ttl...\n",
      "Processing output_25.ttl...\n",
      "Processing output_26.ttl...\n",
      "Processing output_27.ttl...\n",
      "Processing output_28.ttl...\n",
      "Processing output_29.ttl...\n",
      "Processing output_2A.ttl...\n",
      "Processing output_2B.ttl...\n",
      "Processing output_30.ttl...\n",
      "Processing output_31.ttl...\n",
      "Processing output_32.ttl...\n",
      "Processing output_33.ttl...\n",
      "Processing output_34.ttl...\n",
      "Processing output_35.ttl...\n",
      "Processing output_36.ttl...\n",
      "Processing output_37.ttl...\n",
      "Processing output_38.ttl...\n",
      "Processing output_39.ttl...\n",
      "Processing output_40.ttl...\n",
      "Processing output_41.ttl...\n",
      "Processing output_42.ttl...\n",
      "Processing output_43.ttl...\n",
      "Processing output_44.ttl...\n",
      "Processing output_45.ttl...\n",
      "Processing output_46.ttl...\n",
      "Processing output_47.ttl...\n",
      "Processing output_48.ttl...\n",
      "Processing output_49.ttl...\n",
      "Processing output_50.ttl...\n",
      "Processing output_51.ttl...\n",
      "Processing output_52.ttl...\n",
      "Processing output_53.ttl...\n",
      "Processing output_54.ttl...\n",
      "Processing output_55.ttl...\n",
      "Processing output_56.ttl...\n",
      "Processing output_57.ttl...\n",
      "Processing output_58.ttl...\n",
      "Processing output_59.ttl...\n",
      "Processing output_60.ttl...\n",
      "Processing output_61.ttl...\n",
      "Processing output_62.ttl...\n",
      "Processing output_63.ttl...\n",
      "Processing output_64.ttl...\n",
      "Processing output_65.ttl...\n",
      "Processing output_66.ttl...\n",
      "Processing output_67.ttl...\n",
      "Processing output_68.ttl...\n",
      "Processing output_69.ttl...\n",
      "Processing output_70.ttl...\n",
      "Processing output_71.ttl...\n",
      "Processing output_72.ttl...\n",
      "Processing output_73.ttl...\n",
      "Processing output_74.ttl...\n",
      "Processing output_75.ttl...\n",
      "Processing output_76.ttl...\n",
      "Processing output_77.ttl...\n",
      "Processing output_78.ttl...\n",
      "Processing output_79.ttl...\n",
      "Processing output_80.ttl...\n",
      "Processing output_81.ttl...\n",
      "Processing output_82.ttl...\n",
      "Processing output_83.ttl...\n",
      "Processing output_84.ttl...\n",
      "Processing output_85.ttl...\n",
      "Processing output_86.ttl...\n",
      "Processing output_87.ttl...\n",
      "Processing output_88.ttl...\n",
      "Processing output_89.ttl...\n",
      "Processing output_90.ttl...\n",
      "Processing output_91.ttl...\n",
      "Processing output_92.ttl...\n",
      "Processing output_93.ttl...\n",
      "Processing output_94.ttl...\n",
      "Processing output_95.ttl...\n",
      "Processing output_971.ttl...\n",
      "Processing output_972.ttl...\n",
      "Processing output_973.ttl...\n",
      "Processing output_974.ttl...\n",
      "Processing output_976.ttl...\n",
      "\n",
      "Merge completed!\n",
      "Output file: output_merged.ttl\n",
      "Total prefixes: 6\n",
      "Total triple lines: 514619\n",
      "File size: 20,723,450 bytes (19.76 MB)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from rdflib import Graph\n",
    "\n",
    "def merge_ttl_files_with_dedup(output_dir='outputs', merged_file='output_merged.ttl'):\n",
    "    \"\"\"\n",
    "    Merge all TTL files from output directory into one file with automatic deduplication.\n",
    "    Uses RDFLib to parse and merge graphs, which automatically handles duplicate triples.\n",
    "    \"\"\"\n",
    "    # Get all .ttl files in the outputs directory\n",
    "    output_path = Path(output_dir)\n",
    "    ttl_files = sorted(output_path.glob('output_*.ttl'))\n",
    "    \n",
    "    if not ttl_files:\n",
    "        print(\"No output files found to merge.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(ttl_files)} files to merge...\")\n",
    "    \n",
    "    # Create a new graph for merging\n",
    "    merged_graph = Graph()\n",
    "    \n",
    "    # Load and merge all files\n",
    "    for ttl_file in ttl_files:\n",
    "        print(f\"Loading {ttl_file.name}...\")\n",
    "        try:\n",
    "            g = Graph()\n",
    "            g.parse(ttl_file, format='turtle')\n",
    "            print(f\"  → {len(g):,} triples\")\n",
    "            \n",
    "            # Merge into main graph (automatically deduplicates)\n",
    "            merged_graph += g\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal unique triples after merge: {len(merged_graph):,}\")\n",
    "    \n",
    "    # Serialize the merged graph\n",
    "    print(f\"Writing merged file to {merged_file}...\")\n",
    "    merged_graph.serialize(destination=merged_file, format='turtle')\n",
    "    \n",
    "    # Stats\n",
    "    file_size = os.path.getsize(merged_file)\n",
    "    print(f\"\\n✓ Merge completed!\")\n",
    "    print(f\"Output file: {merged_file}\")\n",
    "    print(f\"Unique triples: {len(merged_graph):,}\")\n",
    "    print(f\"File size: {file_size:,} bytes ({file_size / 1024 / 1024:.2f} MB)\")\n",
    "\n",
    "# Execute merge with deduplication\n",
    "merge_ttl_files_with_dedup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ed0cf6",
   "metadata": {},
   "source": [
    "## Déduplication d'un fichier TTL existant\n",
    "\n",
    "Si vous avez déjà un fichier merged avec des doublons, utilisez la cellule suivante pour le nettoyer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34e2660a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement de output_merged.ttl...\n",
      "Taille originale: 19.76 MB\n",
      "Triples uniques: 507,360\n",
      "\n",
      "Écriture du fichier dédupliqué...\n",
      "\n",
      "✓ Déduplication terminée!\n",
      "Fichier de sortie: output_merged_deduplicated.ttl\n",
      "Taille finale: 20.33 MB\n",
      "Réduction: -0.57 MB (-2.9%)\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph\n",
    "from pathlib import Path\n",
    "\n",
    "input_file = 'output_merged.ttl'\n",
    "output_file = 'output_merged_deduplicated.ttl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ed65fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = Path(input_file)\n",
    "input_size = input_path.stat().st_size / (1024 * 1024)  # MB\n",
    "g = Graph()\n",
    "g.parse(input_file, format='turtle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cffd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.serialize(destination=output_file, format='turtle')\n",
    "output_path = Path(output_file)\n",
    "output_size = output_path.stat().st_size / (1024 * 1024)  # MB\n",
    "reduction = ((input_size - output_size) / input_size) * 100\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
